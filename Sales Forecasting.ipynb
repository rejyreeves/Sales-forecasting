{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "703df4a3-6144-4771-a713-ee4c5317a486",
   "metadata": {},
   "source": [
    "<h2>Sales Forecasting<h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed2958e-3ae8-46d5-8753-4726e641b357",
   "metadata": {},
   "source": [
    "<h3> Data Science <h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b2d2bd-d76a-43d1-8aa5-c768ffbf3e41",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    \n",
    "a.Import the datasets into the Python environment\n",
    "    \n",
    "b.Examine the dataset's shape and structure, and look out for any outlier\n",
    "    \n",
    "c.Merge the datasets into a single dataset that includes the date, item id, price, item count, item names, kcal values, store id, and store name<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4dc2cec-df27-44e5-a4e0-79c1bf2955a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "import pandas as pd\n",
    "items_df = pd.read_csv('items.csv')\n",
    "sales_df = pd.read_csv('sales.csv')\n",
    "restaurants_df = pd.read_csv('resturants.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5219f0c-98e0-4c4a-9280-379681e19a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f525ba02-9030-44c0-affe-50395d42efae",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6f2c6de-565f-45c6-aa57-abf1d3c87763",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dba973ad-becf-4c5e-b3be-3977c8d8218c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge sales with items on the item name\n",
    "merged_sales_items = pd.merge(sales_df, items_df, left_on='item_id', right_on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30ef05be-9626-4dac-a667-0dfa27e87c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the result with restaurants on the store ID\n",
    "final_merged_df = pd.merge(merged_sales_items, restaurants_df, left_on='store_id', right_on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f22dc69-adc4-49c7-b982-7a346aa597fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the relevant columns for the final merged dataset\n",
    "final_dataset = final_merged_df[['date', 'id_x', 'price', 'item_count', 'name_x', 'kcal', 'store_id', 'name_y']]\n",
    "\n",
    "# Rename columns for clarity\n",
    "final_dataset.columns = ['date', 'item_id', 'price', 'item_count', 'item_name', 'kcal', 'store_id', 'store_name']\n",
    "\n",
    "# Display the final dataset\n",
    "final_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992f3fb4-ca39-447c-b702-47a1c0483c6d",
   "metadata": {},
   "source": [
    "<h3>Exploratory data analysis:<h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3f2df4-a070-469b-8c29-af586adae5c6",
   "metadata": {},
   "source": [
    "<h4>a.Examine the overall date wise sales to understand the pattern<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02b6269f-ebe5-415a-8093-0f12f0e1defc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Convert the 'date' column to datetime format\n",
    "final_dataset['date'] = pd.to_datetime(sales_df['date'])\n",
    "# Aggregate total item counts and total sales (revenue) by date\n",
    "datewise_sales = final_dataset.groupby('date').agg(\n",
    "    total_items_sold=('item_count', 'sum'),\n",
    "    total_sales=('price', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Display the aggregated data\n",
    "print(datewise_sales)\n",
    "\n",
    "# Plot total items sold over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(datewise_sales['date'], datewise_sales['total_items_sold'], marker='o', label='Total Items Sold')\n",
    "plt.title('Date-wise Total Items Sold')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Total Items Sold')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot total sales (revenue) over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(datewise_sales['date'], datewise_sales['total_sales'], marker='o', color='orange', label='Total Sales (Revenue)')\n",
    "plt.title('Date-wise Total Sales')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Total Sales (Revenue)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092b1c1b-c6c8-4c5c-b8f7-875f14d005d2",
   "metadata": {},
   "source": [
    "<h4>b.Find out how sales fluctuate across different days of the week<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe7434c9-41a4-41b6-aaaf-0b7cfab50ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract the day of the week from the 'date' column (0=Monday, 6=Sunday)\n",
    "final_dataset['day_of_week'] = final_dataset['date'].dt.dayofweek\n",
    "\n",
    "# Map the day of the week to their names\n",
    "day_names = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', 4: 'Friday', 5: 'Saturday', 6: 'Sunday'}\n",
    "final_dataset['day_name'] = sales_df['day_of_week'].map(day_names)\n",
    "\n",
    "# Aggregate total item counts and total sales by day of the week\n",
    "sales_by_day = final_dataset.groupby('day_name').agg(\n",
    "    total_items_sold=('item_count', 'sum'),\n",
    "    total_sales=('price', 'sum')\n",
    ").reindex(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']).reset_index()\n",
    "\n",
    "# Display the aggregated data\n",
    "print(sales_by_day)\n",
    "\n",
    "# Plot total items sold by day of the week\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(sales_by_day['day_name'], sales_by_day['total_items_sold'], color='skyblue')\n",
    "plt.title('Total Items Sold by Day of the Week')\n",
    "plt.xlabel('Day of the Week')\n",
    "plt.ylabel('Total Items Sold')\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n",
    "\n",
    "# Plot total sales (revenue) by day of the week\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(sales_by_day['day_name'], sales_by_day['total_sales'], color='orange')\n",
    "plt.title('Total Sales (Revenue) by Day of the Week')\n",
    "plt.xlabel('Day of the Week')\n",
    "plt.ylabel('Total Sales (Revenue)')\n",
    "plt.grid(axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5067594b-734c-41a1-aebb-14a881396109",
   "metadata": {},
   "source": [
    "<h4>c.Look for any noticeable trends in the sales data for different months of the year<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "786d50c4-76f4-44c0-bae1-ef3a22f530cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract year and month\n",
    "final_dataset['year'] = final_dataset['date'].dt.year\n",
    "final_dataset['month'] = final_dataset['date'].dt.month\n",
    "\n",
    "# Aggregate sales data by year and month\n",
    "monthly_sales = final_dataset.groupby(['year', 'month']).agg(\n",
    "    total_sales=('price', 'sum'),\n",
    "    total_item_count=('item_count', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Plot total sales per month\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(monthly_sales['month'].astype(str) + '-' + monthly_sales['year'].astype(str), \n",
    "         monthly_sales['total_sales'], marker='o', linestyle='-')\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Month-Year')\n",
    "plt.ylabel('Total Sales')\n",
    "plt.title('Total Sales per Month')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot total item count per month\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(monthly_sales['month'].astype(str) + '-' + monthly_sales['year'].astype(str), \n",
    "         monthly_sales['total_item_count'], marker='o', linestyle='-')\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Month-Year')\n",
    "plt.ylabel('Total Item Count')\n",
    "plt.title('Total Item Count per Month')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3904cf97-9d21-47bf-84e7-2dfd02753748",
   "metadata": {},
   "source": [
    "<h4>d.Examine the sales distribution across different quarters averaged over the years. Identify any noticeable patterns.<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2b6710c-9f46-4fc1-82e8-f8e5ddb97ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset['quarter'] = final_dataset['date'].dt.to_period('Q')\n",
    "\n",
    "# Aggregate sales data by quarter\n",
    "quarterly_sales = final_dataset.groupby(['year', 'quarter']).agg(\n",
    "    total_sales=('price', 'sum'),\n",
    "    total_item_count=('item_count', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate average sales and item count per quarter across years\n",
    "quarterly_avg_sales = quarterly_sales.groupby('quarter').agg(\n",
    "    avg_sales=('total_sales', 'mean'),\n",
    "    avg_item_count=('total_item_count', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "# Plot average sales per quarter\n",
    "plt.figure(figsize=(12, 6))\n",
    "quarters = quarterly_avg_sales['quarter'].astype(str)\n",
    "plt.plot(quarters, quarterly_avg_sales['avg_sales'], marker='o', linestyle='-')\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Quarter')\n",
    "plt.ylabel('Average Sales')\n",
    "plt.title('Average Sales per Quarter')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot average item count per quarter\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(quarters, quarterly_avg_sales['avg_item_count'], marker='o', linestyle='-')\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Quarter')\n",
    "plt.ylabel('Average Item Count')\n",
    "plt.title('Average Item Count per Quarter')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa69c6cc-bb6b-4a2f-b08a-e25e9df96d1b",
   "metadata": {},
   "source": [
    "<h4>e.Compare the performances of the different restaurants. Find out which restaurant had the most sales and look at the sales for each restaurant across different years, months, and days.<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "45cee0c1-a36f-4687-8cc2-97954297430d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate total sales by restaurant\n",
    "restaurant_sales = final_dataset.groupby('store_name').agg(\n",
    "    total_sales=('price', 'sum'),\n",
    "    total_item_count=('item_count', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Find the restaurant with the most sales\n",
    "top_restaurant = restaurant_sales.loc[restaurant_sales['total_sales'].idxmax()]\n",
    "\n",
    "print(f\"Restaurant with the most sales: {top_restaurant['store_name']}\")\n",
    "print(f\"Total Sales: ${top_restaurant['total_sales']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ceb830a8-bb26-48e1-84a4-fca7b332b730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract year, month, and quarter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert the 'date' column to datetime format\n",
    "final_dataset['date'] = pd.to_datetime(final_dataset['date'])\n",
    "final_dataset['year'] = final_dataset['date'].dt.year\n",
    "final_dataset['month'] = final_dataset['date'].dt.month\n",
    "final_dataset['quarter'] = final_dataset['date'].dt.to_period('Q')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75d1d0ae-d2d9-497a-9a1f-549cc209c5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate sales data by restaurant and year\n",
    "yearly_sales = final_dataset.groupby(['store_name', 'year']).agg(\n",
    "    total_sales=('price', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Aggregate sales data by restaurant and month\n",
    "monthly_sales = final_dataset.groupby(['store_name', 'year', 'month']).agg(\n",
    "    total_sales=('price', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Aggregate sales data by restaurant and day\n",
    "daily_sales = final_dataset.groupby(['store_name', 'date']).agg(\n",
    "    total_sales=('price', 'sum')\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f160f481-c072-4820-aa29-5eac12d5f58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'final_merged_df' is your merged dataset\n",
    "\n",
    "# Convert 'date' column to datetime\n",
    "final_dataset['date'] = pd.to_datetime(final_dataset['date'])\n",
    "\n",
    "# Extract month from the date\n",
    "final_dataset['month'] = final_dataset['date'].dt.month\n",
    "\n",
    "# Group by month and sum the item_count\n",
    "monthly_sales = final_dataset.groupby('month')['item_count'].sum()\n",
    "\n",
    "# Plot the trends\n",
    "plt.figure(figsize=(10,6))\n",
    "monthly_sales.plot(kind='bar', color='skyblue')\n",
    "plt.title('Total Item Sales by Month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Total Items Sold')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2173252a-b12b-4a16-9d38-81ee426c8d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert 'date' column to datetime if not done yet\n",
    "final_dataset['date'] = pd.to_datetime(final_dataset['date'])\n",
    "\n",
    "# Extract quarter from the date\n",
    "final_dataset['quarter'] = final_dataset['date'].dt.quarter\n",
    "\n",
    "# Group by quarter and calculate average item_count\n",
    "quarterly_sales = final_dataset.groupby('quarter')['item_count'].mean()\n",
    "\n",
    "# Plot the sales distribution across quarters\n",
    "plt.figure(figsize=(10,6))\n",
    "quarterly_sales.plot(kind='bar', color='orange')\n",
    "plt.title('Average Item Sales by Quarter')\n",
    "plt.xlabel('Quarter')\n",
    "plt.ylabel('Average Items Sold')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1646aa-e158-44cb-8851-9b41a418c062",
   "metadata": {},
   "source": [
    "<h4>f.\n",
    "Identify the most popular items overall and the stores where they are being sold. Also, find out the most popular item at each store<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "16e189b1-3026-4253-86a9-8cbe8ba0307a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Step 1: Identify the most popular items overall\n",
    "# Group by 'name' (item name) and sum up the 'item_count' to get total sales for each item\n",
    "most_popular_items = final_dataset.groupby('item_name')['item_count'].sum().reset_index()\n",
    "\n",
    "# Sort items by total sales in descending order to find the most popular items\n",
    "most_popular_items = most_popular_items.sort_values(by='item_count', ascending=False)\n",
    "\n",
    "print(\"Most Popular Items Overall:\")\n",
    "print(most_popular_items.head())  # Display the top 5 popular items\n",
    "\n",
    "# Step 2: Identify the stores where the most popular items are being sold\n",
    "# Group by 'name' and 'store_name' to find the total sales of each item in each store\n",
    "item_sales_by_store = final_dataset.groupby(['item_name', 'store_name'])['item_count'].sum().reset_index()\n",
    "\n",
    "# Merge with the most popular items to find the stores where these items are being sold\n",
    "top_item_stores = pd.merge(most_popular_items, item_sales_by_store, on='item_name')\n",
    "\n",
    "print(\"Stores Selling the Most Popular Items:\")\n",
    "print(top_item_stores.head())  # Display top results\n",
    "\n",
    "# Step 3: Find the most popular item at each store\n",
    "# Group by 'store_name' and 'name', then sum 'item_count' to find total sales for each item at each store\n",
    "store_item_sales = final_dataset.groupby(['store_name', 'item_name'])['item_count'].sum().reset_index()\n",
    "\n",
    "# Sort by 'store_name' and 'item_count' to find the most popular item for each store\n",
    "store_item_sales = store_item_sales.sort_values(by=['store_name', 'item_count'], ascending=[True, False])\n",
    "\n",
    "# Drop duplicates to keep only the most popular item per store\n",
    "most_popular_item_per_store = store_item_sales.drop_duplicates(subset=['store_name'], keep='first')\n",
    "\n",
    "print(\"Most Popular Item at Each Store:\")\n",
    "print(most_popular_item_per_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c4a3cc96-d8ce-48ab-b4cd-a3503637ce7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot the top 10 most popular items\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=most_popular_items.head(10), x='item_count', y='item_name', palette='viridis')\n",
    "plt.title('Top 10 Most Popular Items (Overall)', fontsize=16)\n",
    "plt.xlabel('Total Sales (Item Count)', fontsize=12)\n",
    "plt.ylabel('Item Name', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "983b4d9f-6661-4359-b5f3-eafe16b8ef17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot total sales of the top 5 items in each store\n",
    "top_item_stores = top_item_stores.groupby('store_name').head(5)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=top_item_stores, x='store_name', y='item_count_x', hue='item_name', palette='coolwarm')\n",
    "plt.title('Total Sales of Most Popular Items by Store', fontsize=16)\n",
    "plt.xlabel('Store Name', fontsize=12)\n",
    "plt.ylabel('Total Sales (Item Count)', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Item Name', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "74408165-7688-4d7a-a549-9e28382b8abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the most popular item in each store\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=most_popular_item_per_store, x='store_name', y='item_count', hue='item_name', palette='magma')\n",
    "plt.title('Most Popular Item in Each Store', fontsize=16)\n",
    "plt.xlabel('Store Name', fontsize=12)\n",
    "plt.ylabel('Total Sales (Item Count)', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Item Name', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8b977e1a-e0c9-40a6-8047-598fbdd782c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for pie chart (top 5 most popular items)\n",
    "top_5_items = most_popular_items.head(5)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(top_5_items['item_count'], labels=top_5_items['item_name'], autopct='%1.1f%%', startangle=140, colors=sns.color_palette('Set2'))\n",
    "plt.title('Sales Distribution of Top 5 Most Popular Items', fontsize=16)\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "31058264-e0fa-4609-affc-15c786d99a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pivot table for heatmap\n",
    "heatmap_data = item_sales_by_store.pivot('store_name', 'item_name', 'item_count')\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(heatmap_data, cmap='YlGnBu', annot=True, fmt='.0f', linewidths=0.5)\n",
    "plt.title('Heatmap of Item Sales by Store', fontsize=16)\n",
    "plt.xlabel('Item Name', fontsize=12)\n",
    "plt.ylabel('Store Name', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "38ff3d9d-d4a1-491c-b6d6-670502312f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Group by item name to calculate total sales (item_count) for each item\n",
    "popular_items = final_dataset.groupby('item_name')['item_count'].sum().sort_values(ascending=False)\n",
    "\n",
    "# Find the most popular item overall\n",
    "most_popular_item = popular_items.idxmax()\n",
    "most_popular_item_sales = popular_items.max()\n",
    "\n",
    "print(f\"The most popular item overall is: {most_popular_item} with {most_popular_item_sales} total items sold.\")\n",
    "\n",
    "# Find which stores sell the most popular items\n",
    "stores_with_popular_items = final_dataset[final_dataset['item_name'] == most_popular_item]['store_name'].unique()\n",
    "print(f\"Stores selling the most popular item ({most_popular_item}): {stores_with_popular_items}\")\n",
    "\n",
    "# Now, to find the most popular item at each store\n",
    "popular_items_by_store = final_dataset.groupby(['item_name'])['item_count'].sum()\n",
    "\n",
    "# Reset the index for easier manipulation\n",
    "popular_items_by_store = popular_items_by_store.reset_index()\n",
    "\n",
    "# Find the most popular item for each store\n",
    "most_popular_item_by_store = popular_items_by_store.loc[popular_items_by_store.groupby('item_name')['item_count'].idxmax()]\n",
    "\n",
    "print(\"Most popular item at each store:\")\n",
    "print(most_popular_item_by_store)\n",
    "\n",
    "# Optional: If you want to plot the most popular items overall\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "popular_items.head(10).plot(kind='bar', color='green')\n",
    "plt.title('Top 10 Most Popular Items')\n",
    "plt.xlabel('Item')\n",
    "plt.ylabel('Total Items Sold')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede03791-72a8-4a21-b318-cd199f9a940b",
   "metadata": {},
   "source": [
    "<h4>g.Determine if the store with the highest sales volume is also making the most money per day<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7c4dedd6-5f3f-4ba7-99a5-4147003a4703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert 'date' column to datetime if not done already\n",
    "final_dataset['date'] = pd.to_datetime(final_dataset['date'])\n",
    "\n",
    "# Step 1: Calculate total sales volume for each store\n",
    "store_sales_volume = final_dataset.groupby('store_name')['item_count'].sum()\n",
    "\n",
    "# Step 2: Find the store with the highest sales volume\n",
    "top_store_by_sales_volume = store_sales_volume.idxmax()\n",
    "top_store_sales_volume = store_sales_volume.max()\n",
    "\n",
    "print(f\"The store with the highest sales volume is: {top_store_by_sales_volume} with {top_store_sales_volume} total items sold.\")\n",
    "\n",
    "# Step 3: Calculate daily revenue for each store (price * item_count)\n",
    "final_dataset['daily_revenue'] = final_dataset['price'] * final_dataset['item_count']\n",
    "\n",
    "# Step 4: Group by store and date to calculate total daily revenue\n",
    "daily_revenue_by_store = final_dataset.groupby(['store_name', 'date'])['daily_revenue'].sum()\n",
    "\n",
    "# Step 5: Calculate the average daily revenue for each store\n",
    "average_daily_revenue_by_store = daily_revenue_by_store.groupby('store_name').mean()\n",
    "\n",
    "# Find the store with the highest average daily revenue\n",
    "top_store_by_daily_revenue = average_daily_revenue_by_store.idxmax()\n",
    "top_store_daily_revenue = average_daily_revenue_by_store.max()\n",
    "\n",
    "print(f\"The store with the highest average daily revenue is: {top_store_by_daily_revenue} with an average daily revenue of {top_store_daily_revenue:.2f}\")\n",
    "\n",
    "# Comparison\n",
    "if top_store_by_sales_volume == top_store_by_daily_revenue:\n",
    "    print(f\"The store with the highest sales volume, {top_store_by_sales_volume}, also makes the most money per day.\")\n",
    "else:\n",
    "    print(f\"The store with the highest sales volume, {top_store_by_sales_volume}, does not make the most money per day. The store with the highest daily revenue is {top_store_by_daily_revenue}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad86e56-754d-44a5-b2f4-557d2a8aff70",
   "metadata": {},
   "source": [
    "<h4>h.Identify the most expensive item at each restaurant and find out its calorie count<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "febc0763-bba7-4cf6-bd7e-b62978899685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Group by store_name and find the most expensive item at each store\n",
    "most_expensive_items = final_dataset.groupby(['store_name', 'item_name']).agg({'price': 'max', 'kcal': 'first'}).reset_index()\n",
    "\n",
    "# Step 2: Sort by 'store_name' and 'price' to get the most expensive item for each store\n",
    "most_expensive_items = most_expensive_items.sort_values(by=['store_name', 'price'], ascending=[True, False])\n",
    "\n",
    "# Step 3: Drop duplicates to keep only the most expensive item per store\n",
    "most_expensive_item_per_store = most_expensive_items.drop_duplicates(subset=['store_name'], keep='first')\n",
    "\n",
    "# Step 4: Display the results (most expensive item and its calorie count at each store)\n",
    "print(\"Most Expensive Item at Each Restaurant (and its Calorie Count):\")\n",
    "print(most_expensive_item_per_store[['store_name', 'item_name', 'price', 'kcal']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d538711-2f0a-46e2-ab9b-d2bca7f13c95",
   "metadata": {},
   "source": [
    "<h3> Machine Learning <h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfcfd12-f0b8-49cb-9530-beeac6fbc5a4",
   "metadata": {},
   "source": [
    "<h4>a.\n",
    "Build and compare linear regression, random forest, and XGBoost models for predictions<h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222ff8c8-76c3-4f4e-bd57-389e03866914",
   "metadata": {},
   "source": [
    "<h5>\n",
    "    \n",
    "•Generate necessary features for the development of these models, like day of the week, quarter of the year, month, year, day of the month and so on\n",
    "    \n",
    "•Use the data from the last six months as the testing data\n",
    "    \n",
    "•Compute the root mean square error (RMSE) values for each model to compare their performances\n",
    "    \n",
    "•Use the best-performing models to make a forecast for the next year<h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d1da618a-c147-4c59-83d6-16a6533f9b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 1: Feature Engineering\n",
    "final_dataset['date'] = pd.to_datetime(final_dataset['date'])\n",
    "final_dataset['year'] = final_dataset['date'].dt.year\n",
    "final_dataset['month'] = final_dataset['date'].dt.month\n",
    "final_dataset['day'] = final_dataset['date'].dt.day\n",
    "final_dataset['day_of_week'] = final_dataset['date'].dt.dayofweek\n",
    "final_dataset['quarter'] = final_dataset['date'].dt.quarter\n",
    "\n",
    "# Target: total sales revenue\n",
    "final_dataset['sales_revenue'] = final_dataset['price'] * final_dataset['item_count']\n",
    "\n",
    "# Step 2: Prepare Training and Testing Data (last 6 months for testing)\n",
    "train_data = final_dataset[final_dataset['date'] < final_dataset['date'].max() - pd.DateOffset(months=6)]\n",
    "test_data = final_dataset[final_dataset['date'] >= final_dataset['date'].max() - pd.DateOffset(months=6)]\n",
    "\n",
    "# Features for the models\n",
    "features = ['year', 'month', 'day', 'day_of_week', 'quarter']\n",
    "X_train = train_data[features]\n",
    "y_train = train_data['sales_revenue']\n",
    "X_test = test_data[features]\n",
    "y_test = test_data['sales_revenue']\n",
    "\n",
    "# Standardizing features (especially important for linear regression)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 3: Build Models\n",
    "# Linear Regression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train_scaled, y_train)\n",
    "y_pred_lr = lin_reg.predict(X_test_scaled)\n",
    "rmse_lr = np.sqrt(mean_squared_error(y_test, y_pred_lr))\n",
    "\n",
    "# Random Forest\n",
    "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_reg.fit(X_train, y_train)\n",
    "y_pred_rf = rf_reg.predict(X_test)\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "\n",
    "# XGBoost\n",
    "xgb_reg = XGBRegressor(n_estimators=100, random_state=42)\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_reg.predict(X_test)\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\n",
    "\n",
    "# Step 4: Compare RMSE for each model\n",
    "print(f\"RMSE - Linear Regression: {rmse_lr}\")\n",
    "print(f\"RMSE - Random Forest: {rmse_rf}\")\n",
    "print(f\"RMSE - XGBoost: {rmse_xgb}\")\n",
    "\n",
    "# Step 5: Forecast for the Next Year using the best model (lowest RMSE)\n",
    "# Use XGBoost if it has the lowest RMSE as an example\n",
    "best_model = xgb_reg if rmse_xgb < rmse_lr and rmse_xgb < rmse_rf else (rf_reg if rmse_rf < rmse_lr else lin_reg)\n",
    "\n",
    "# Prepare the next year's data for forecasting\n",
    "future_dates = pd.date_range(start=final_dataset['date'].max() + pd.DateOffset(days=1), periods=365, freq='D')\n",
    "future_df = pd.DataFrame({'date': future_dates})\n",
    "future_df['year'] = future_df['date'].dt.year\n",
    "future_df['month'] = future_df['date'].dt.month\n",
    "future_df['day'] = future_df['date'].dt.day\n",
    "future_df['day_of_week'] = future_df['date'].dt.dayofweek\n",
    "future_df['quarter'] = future_df['date'].dt.quarter\n",
    "\n",
    "X_future = future_df[features]\n",
    "X_future_scaled = scaler.transform(X_future)  # Scale the future data as well\n",
    "\n",
    "# Predict sales for the next year\n",
    "future_sales_forecast = best_model.predict(X_future_scaled)\n",
    "\n",
    "# Step 6: Add forecasted sales to the future dataframe\n",
    "future_df['forecasted_sales_revenue'] = future_sales_forecast\n",
    "\n",
    "# Display the forecasted sales for the next year\n",
    "future_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b007baf3-e1c5-493e-9979-a4649fbbef73",
   "metadata": {},
   "source": [
    "<h3> Deep Learning <h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd365731-9e5f-42ee-ac2a-283aae52b928",
   "metadata": {},
   "source": [
    "<h4>Forecasting using deep learning algorithms:<h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c7d8e5-34cc-417b-a25e-c2b2b677d142",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    \n",
    "    a.Use sales amount for predictions instead of item count\n",
    "    \n",
    "    b.Build a long short-term memory (LSTM) model for predictions\n",
    "    \n",
    "        •Define the train and test series\n",
    "        •Generate synthetic data for the last 12 months\n",
    "        •Build and train an LSTM model\n",
    "        •Use the model to make predictions for the test data\n",
    "    \n",
    "    c.Calculate the mean absolute percentage error (MAPE) and comment on the model's performance\n",
    "    \n",
    "    d.Develop another model using the entire series for training, and use it to forecast for the next three months<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4733732f-9d2d-4b37-afce-6a9ddbd9e036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "# Step 1: Prepare the Data (sales_revenue for predictions)\n",
    "final_dataset['date'] = pd.to_datetime(final_dataset['date'])\n",
    "final_dataset['sales_revenue'] = final_dataset['price'] * final_dataset['item_count']\n",
    "\n",
    "# Step 2: Sort by date and use the 'sales_revenue' column for time series prediction\n",
    "final_dataset = final_dataset[['date', 'sales_revenue']].sort_values(by='date')\n",
    "final_dataset.set_index('date', inplace=True)\n",
    "\n",
    "# Step 3: Split the data into train and test (last 12 months as test)\n",
    "split_date = final_dataset.index.max() - pd.DateOffset(months=12)\n",
    "train_data = final_dataset[final_dataset.index < split_date]\n",
    "test_data = final_dataset[final_dataset.index >= split_date]\n",
    "\n",
    "# Normalize the data for LSTM\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "train_scaled = scaler.fit_transform(train_data)\n",
    "test_scaled = scaler.transform(test_data)\n",
    "\n",
    "# Step 4: Prepare the data for LSTM\n",
    "def create_sequences(data, time_steps=30):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data[i:i + time_steps])\n",
    "        y.append(data[i + time_steps])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "time_steps = 30  # Using last 30 days to predict the next day\n",
    "X_train, y_train = create_sequences(train_scaled, time_steps)\n",
    "X_test, y_test = create_sequences(test_scaled, time_steps)\n",
    "\n",
    "# Reshape the data to be accepted by LSTM (samples, time steps, features)\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Step 5: Build the LSTM Model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=50, return_sequences=True, input_shape=(time_steps, 1)))\n",
    "model.add(LSTM(units=50))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Step 6: Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32)\n",
    "\n",
    "# Step 7: Make Predictions for the Test Data\n",
    "y_pred_scaled = model.predict(X_test)\n",
    "\n",
    "# Inverse the scaling to get the original scale\n",
    "y_pred = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_actual = scaler.inverse_transform(y_test)\n",
    "\n",
    "# Step 8: Calculate MAPE\n",
    "mape = mean_absolute_percentage_error(y_test_actual, y_pred)\n",
    "print(f\"Mean Absolute Percentage Error (MAPE): {mape * 100:.2f}%\")\n",
    "\n",
    "# Plot the actual vs predicted sales\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(test_data.index[time_steps:], y_test_actual, color='blue', label='Actual Sales')\n",
    "plt.plot(test_data.index[time_steps:], y_pred, color='red', label='Predicted Sales')\n",
    "plt.title('LSTM Model - Actual vs Predicted Sales')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales Revenue')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Step 9: Train a model with the entire series for forecasting the next 3 months\n",
    "# Combine the train and test data for this model\n",
    "full_data_scaled = scaler.fit_transform(final_dataset)\n",
    "\n",
    "# Recreate sequences for the full dataset\n",
    "X_full, y_full = create_sequences(full_data_scaled, time_steps)\n",
    "X_full = X_full.reshape((X_full.shape[0], X_full.shape[1], 1))\n",
    "\n",
    "# Train the LSTM on the full dataset\n",
    "model_full = Sequential()\n",
    "model_full.add(LSTM(units=50, return_sequences=True, input_shape=(time_steps, 1)))\n",
    "model_full.add(LSTM(units=50))\n",
    "model_full.add(Dense(1))\n",
    "model_full.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model_full.fit(X_full, y_full, epochs=50, batch_size=32)\n",
    "\n",
    "# Step 10: Forecast for the next 3 months (90 days)\n",
    "future_days = 90\n",
    "X_input = full_data_scaled[-time_steps:]  # Use the last 30 days to predict the future\n",
    "X_input = X_input.reshape((1, time_steps, 1))\n",
    "\n",
    "forecasted_sales = []\n",
    "\n",
    "for _ in range(future_days):\n",
    "    prediction_scaled = model_full.predict(X_input)\n",
    "    forecasted_sales.append(prediction_scaled[0, 0])\n",
    "    \n",
    "    # Update the input sequence with the new prediction\n",
    "    X_input = np.append(X_input[:, 1:, :], [[prediction_scaled]], axis=1)\n",
    "\n",
    "# Inverse scale the forecasted values\n",
    "forecasted_sales = scaler.inverse_transform(np.array(forecasted_sales).reshape(-1, 1))\n",
    "\n",
    "# Create a future date range for the next 3 months\n",
    "future_dates = pd.date_range(start=final_dataset.index.max() + pd.DateOffset(days=1), periods=future_days)\n",
    "\n",
    "# Plot the forecast\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(future_dates, forecasted_sales, color='green', label='Forecasted Sales for Next 3 Months')\n",
    "plt.title('LSTM Forecast - Next 3 Months')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales Revenue')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad04ff83-5cca-481f-88f7-8ccc1b0ce442",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 [3.10]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
